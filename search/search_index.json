{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome!","text":"<p>Hello! I'm Owen, a CS grad student interested in software development. Welcome to my documentation.</p> <p> GitHub             |             LinkedIn </p>"},{"location":"#selected-projects","title":"Selected Projects","text":"<p>Click a project name!</p> <ul> <li>NeuroLoop - Controlling Neural Synchronization via Closed-Loop Deep Brain Stimulation.</li> <li>FathomView - Underwater Image Enhancement using Deep Learning.</li> <li>FathomMap - Underwater Image Content-Based Image Retrieval.</li> <li>ParallelFFT - Parallel Fast Fourier Transform using OpenMP.</li> <li>SmallTalk - FPGA-Accelerated Speech Recognition using FFT-based Feature Extraction.</li> <li>LoboShell - Unix Shell in C.</li> </ul>"},{"location":"pages/home/fitness/","title":"Fitness","text":"<ul> <li><code>Monday</code> - Push</li> <li><code>Tuesday</code> - Legs I</li> <li><code>Wednesday</code> - Active Recovery</li> <li><code>Thursday</code> - Pull</li> <li><code>Friday</code> - Legs II</li> </ul>"},{"location":"pages/home/fitness/#push","title":"Push","text":"<p>Chest | Shoulders | Triceps</p> <p> est. duration: ~1 hr</p>"},{"location":"pages/home/fitness/#warmup","title":"Warmup","text":"<ul> <li>yoga, hiit cardio, or mobility</li> <li>shoulder circles (wall)</li> <li>steering wheel stretch (bar)</li> </ul>"},{"location":"pages/home/fitness/#workout","title":"Workout","text":"CurrentPrevious Exercise Sets x Reps 1 - incline press (db or bar) 4 x 8 2 - bench press 4 x 8 3 - shoulder press 3 x 10, 10, failure 4 - tricep extension (overhead) 3 x 12 5a - cable crossovers (horizontal) 3 x 5 5b - cable crossovers (low to high) 3 x 5 5c - cable crossovers (high to low) 3 x 5 6 - lateral raise 3 x 20 7 - delt stretches (with or without cable) 3 x 8 per side 8 - tricep pushdowns 3 x 12 9 - diamond pushups 1 x failure 10 - core (MadFit) 5-10 min Exercise Sets x Reps 1a - bench 4 x 6, 8, 10, 12 1b - horizontal cable crossovers 4 x 15 2a - incline dumbbell bench press 4 x 6, 8, 10, 12 2b - low-to-high cable crossovers 4 x 15 3a - dips 4 x 6, 8, 10, 12 3b - high to low cable crossovers 4 x 15 4 - db overhead press 4 x 12, 10, 8, 6 5a - delt cable stretches (side) 3 x 7 per arm 5b - delt cable stretches (front) 3 x 7 per arm 5c - delt cable stretches (back) 3 x 7 per arm 6a - tricep overhead extensions 2 x 12 6b - tricep push downs 2 x 12 Exercises to sample from Exercise Target incline press (db or bar) upper chest, front delts, triceps bench press chest, front delts, triceps shoulder press front and mid delts cable crossovers (horizontal) mid chest cable crossovers (low to high) upper chest cable crossovers (high to low) lower chest lateral raise mid delts, supraspinatus, traps (upper) delt stretches (with or without cable) delts, rotator cuffs tricep extension (overhead) triceps (long head) tricep pushdowns triceps (lateral and medial heads) dips chest, triceps, front delts"},{"location":"pages/home/fitness/#pull","title":"Pull","text":"<p>Back | Biceps | Forearms</p> <p> est. duration: ~1 hr</p>"},{"location":"pages/home/fitness/#warmup_1","title":"Warmup","text":"<ul> <li>yoga or hiit cardio or mobility</li> <li>Workout includes synthesized warmups (eg. face pulls, pull-ups, scap pulldowns).</li> </ul>"},{"location":"pages/home/fitness/#workout_1","title":"Workout","text":"Current Exercise Sets x Reps 1 - face pulls 3 x 15 2 - pullups 3 x failure 3 - scapula pulldowns 3 x 12 4 - lat pulldowns 4 x 10 5 - chest-supported row 3 x 12 6 - cable row 3 x 12 7 - straight-arm pushdowns 3 x 12 8a - db row 3 x 10 8b - db pullovers 2 x 12 9 - hammer curls 3 x 12 10 - preacher curls 3 x 12 11 - db high pulls 3 x 10 12 - Core (MadFit) 5-10 min Exercises to sample from Exercise Target face pulls rear delts, traps, rotator cuffs, scapula pullups lats and upper back scapula pulldowns activates scapula, lats, shoulders (good warm-up) lat pulldowns wide grip: teres major, narrow grip: lats chest-supported row rhomboids, traps, rear delts, lats seated cable rows lats, rear delts, rhomboids straight-arm pushdowns lats db high pulls traps, rear delts cable row lats, rear delts, rhomboids db row lower traps and spinal erectors barbell row lats, rhomboids, traps, rear delts db pullover lats, chest, serratus hammer curls biceps brachialis, forearm preacher curls biceps"},{"location":"pages/home/fitness/#legs-i","title":"Legs I","text":"<p>quads | calves (gastrocnemius)</p> <p> est. duration: ~70 min</p>"},{"location":"pages/home/fitness/#warmup_2","title":"Warmup","text":"<ul> <li>yoga or hiit cardio or mobility</li> <li>backward lunges, tiptoe squats, hip openers (eg. 90-90, pigeon, \u2026)</li> </ul>"},{"location":"pages/home/fitness/#workout_2","title":"Workout","text":"Current Exercise Sets x Reps 1 - squat (heavy) 4 x 8 2 - leg press 4 x 10 3a - goblet squat 3 x 12 3b - split squat 3 x 12 4 - calf raises (standing) 4 x 15 5 - leg extension 4 x 12 6 - Core (MadFit) 5-10 min Exercises to sample from Exercise Target barbell squat (back) quads, glutes, hamstrings, core, lower back leg press quads, glutes, hamstrings, calves goblet squat quads, glutes, core, adductors db split squat quads, glutes, hamstrings, calves, core calf raises (standing) upper calf (gastrocnemius), lower calf (soleus) leg extensions quads (rectus femoris)"},{"location":"pages/home/fitness/#legs-ii","title":"Legs II","text":"<p>glutes | hamstrings | adductors/abductors | calves (soleus)</p> <p> est. duration: todo</p>"},{"location":"pages/home/fitness/#warmup_3","title":"Warmup","text":"<ul> <li>yoga or hiit cardio or mobility</li> <li>backward lunges, tiptoe squats, hip openers (eg. 90-90, pigeon, \u2026)</li> </ul>"},{"location":"pages/home/fitness/#workout_3","title":"Workout","text":"Current Exercise Sets x Reps 1 - hip thrusts 4 x 10 2 - leg curls 4 x 12 3 - squat (light) 3 x 10 4a - abductors 3 x 12 4b - adductors 3 x 12 5 - calf raises (seated) 4 x 15 6 - Core (MadFit) 5-10 min <p>Note</p> <p>Reduce strain on inside semi during leg curls by rotating knees out.</p> Exercises to sample from Exercise Target hip thrusts glutes, hamstrings, core, adductors leg curls hamstrings, calves abductors adductors calf raises (seated) lower calf (soleus), upper calf (gastrocnemius)"},{"location":"pages/home/fitness/#active-recovery","title":"Active Recovery","text":"<p>Run, row, surf, whatever.</p> rowing workouts"},{"location":"pages/home/fitness/#warmup_4","title":"Warmup","text":"<ul> <li>10 minutes low intensity steady state</li> <li>full body mobility routine</li> <li>HIIT Cardio</li> <li>rowing drills (2 minutes each)<ul> <li>leg drive emphasis</li> <li>pause drills</li> <li>high stroke rate sprints</li> </ul> </li> </ul> <p>Computing Max Pace and Pressure</p> <ul> <li> <p>max pressure</p> <ul> <li>Start low, build to perceived max, hold for 10-20 seconds.</li> <li>Repeat 3-5 times with a 2 minute recovery in between.</li> </ul> </li> <li> <p>rest</p> <ul> <li>5-10 minutes easy rowing or light cardio</li> </ul> </li> <li> <p>max pace</p> <ul> <li>Start low, build pace to perceived max, hold for 1 minute.</li> <li>Repeat 3-5 times with a 2 minute recovery in between.</li> </ul> </li> <li> <p>cool down</p> <ul> <li>5-10 minutes easy rowing or light cardio</li> </ul> </li> </ul>"},{"location":"pages/home/fitness/#workout_4","title":"Workout","text":"<p>note: ordered by estimated duration</p> <p>24 min [aerobic] Row 1 x 20 min (almost full pressure but low rating, ~20spm). Row 4 x 1 min at 24/26/28/30. Goal: maintain stroke length and power when tired, taking up the ratings.</p> <p>25 min [aerobic] Row steady state 8K with power 10s at each 1K.</p> <p>29 min [aerobic] Row a time/rating pyramid as follows:</p> Time (min) Rating (spm) 5 20 4 22 3 24 2 26 1 28 2 26 3 24 4 22 5 20 <p>30 min\u00a0[anaerobic, speed] Row 12 x 30s on, 2min off. Row almost flat-out full pressure and high rating for the 30s on.</p> <p>35 min\u00a0[rate change, endurance] Row 4 x 8min rating pyramids with 1min rest between sets: 16-20-18-22-20-24-22-26 18-22-20-24-22-26-24-28 20-24-22-26-24-28-26-30 22-26-24-28-26-30-28-32 Goal:\u00a0controlled rating changes.</p> <p>36 min [speed] Row 3 sets of (on, off) pyramid intervals of (30s on, 30s off), (60s on/ 60s off), (90s on/ 90s off), (60s on/ 60s off), (30s on/ 30s off) with 3min recovery between the sets. Desired ratings: 26-30spm at 90% pressure.</p> <p>34 min [rate changes] Row 3 x 10min, breaking each piece into (4-3-2-1min) segments with rates of (18-20-22-24), (20-22-24-26), and (22-24-26-28) with 2min 50% pressure recovery between sets.</p> <p>36 min [aerobic (threshold)] Row 6 x (5 minutes on, 1 minute off). Keep the rate constant at 24 \u201326 for the \u2018on\u2019 segments. The work intervals should be at \u00be pressure (75%), the \u2018off\u2019 should be no less than half pressure, preferably 60-70%.</p> <p>33 min\u00a0[speed, stamina] Row 5 sets of 5 minutes on, 90 sec off. Use ratings of 24/26/28/26/24 for the \u2018on\u2019 segments, and do these at ~85-95% effort. Use the \u2018off\u2019 time for complete recovery. Work on maintaining form at higher pressure and ratings.</p> <p>35 min [speed, stamina] Row 12 sets of intervals of 20/10/20 strokes. The first five sets are @ 26/22/26 spm, the second five @ 28/24/28, and the last two @30/26/30. Row \u00bc pressure for 30s- 1 min between each.</p> <p>32 min [aerobic] Row 2 x 15 minutes starting around 70% pressure and decreasing your 500m split time by 1-2 seconds every 3 minutes. Row easy for 2 minutes between 15min pieces.</p> <p>37 min [speed] Row 12 x [1 min on/2 min off] intervals switching between stroke rating 26 and 28 for the \u2018on\u2019 intervals (\u2018off\u2019 segments at ~ 22-24 spm). Row 1 minute easy after the first 6 intervals.</p> <p>30 min [aerobic] Row 10/8/6/4/2 minute segments starting at ~ 50-60% pressure for the 10 minute segment and finishing at ~85% for the 2 minute segment.</p> <p>36 min [endurance intervals] Row 3 pyramids of 10/20/30/40/30/20/10 strokes \u2018on\u2019 with 20 strokes \u2018off\u2019 after each \u2018on\u2019 interval (except only 10 strokes after the 10s \u2018on\u2019). The stroke ratings for the \u2018on\u2019 intervals should be 22 for the 1st\u00a0set, 24 for the 2nd\u00a0set, and 26 for the 3rd\u00a0set. The \u2018off\u2019 periods should be no less than half pressure for this endurance workout. Row 2 minutes at \u00bc pressure between the pyramids.</p> <p>35 min [endurance, speed] Finish warmup by rowing 3 minutes at \u00be pressure (~24-26 spm), followed by 2 minutes easy rowing. Then row 8 x (1min on/90s off), followed by 4 x (30s on/1min off). The 1 minute work intervals should be at ~ your 5K race pace, and the 30s intervals at a little below your final sprint pace (~26-30spm). Finish the workout with two \u2018high\u2019 20s (&gt;28spm) before cooling down.</p> <p>35 min [endurance] Finish warmup by rowing 2m/1m/30s at increasing intensity (70-90%) from the 2 min to the 30 sec interval (suggested ratings 22/24/26) followed by 90s easy rowing. Then row 3 sets of [30/60/90/60/30] second intervals \u2018on\u2019 with equal time \u2018off\u2019 after each (i.e. 30s \u2018on\u2019 followed by 30s \u2018off\u2019). Row an additional minute \u2018easy\u2019 after each set.</p> <p>35 min [aerobic, endurance] Row six 5-minute pieces starting at ~70% pressure and bringing the pressure UP and your split time DOWN 1 or 2 seconds at the end of each minute of the piece. Row 2 minutes easy between pieces.</p> <p>35 min [aerobic] Row five 6-minute pieces with the first 3 minutes at around 18-20spm and at 60-65% pressure, and the second 3 minutes at ~20-22spm at ~70% pressure. In other words, take up the pressure with your legs slightly for the second half of each piece; let the rating come up a couple of beats accordingly. Row half pressure for 1 minute after each piece</p> <p>37 min Set up the erg display to show your force curve. Row ~5 minutes while occasionally checking your force curve. Start at your \u2018go forever\u2019 easy aerobic pace for a couple of minutes and try to reach equilibrium with a smooth curve. After 3 minutes, increase the pressure every 30s - you should aim to be at\u00a0~80-85% pressure at the end while maintaining a smooth curve. 5 minutes (Use this exercise to see how your force curve changes as your pressure and rating increases. This may help you figure out what you need to concentrate on with respect to your form when you are at race pressure. For example, do you slack off part way through the stroke as evidenced by a dip in the curve? If so, concentrate on trying to keep the pressure on and make smooth transitions through the drive). Row 2 minutes easy, then row 3 x 8 minutes steady state with \u2018speed\u2019 pickups for 30s every 2 \u00bd minutes. This corresponds to 3 pickups per piece (at 2\u00bd, 5, and 7\u00bd\u00a0 minutes) and is comparable to doing power tens during pieces on the water. These will be at increasing rating through each piece:\u00a0 The first at 24, then 26, then 28, or 26, 28, 30.\u00a0 Or do them at lower ratings if you choose, as long as you increase the rate slightly from one \u2018speed\u2019 interval to the next.\u00a0 The steady state should be at ~ 65-75%, the pickups at ~80-85%. Row easy for 2 minutes between pieces.</p> <p>36 min [endurance intervals] Row 10 x [90s \u2018on\u2019 / 90s \u2018off\u2019]. The \u2018on\u2019 intervals should be at ~80-85%, the \u2018off\u2019 segments at ~ half pressure. It is important to make sure you can complete the workout \u2013 don\u2019t burn out in the first 3 intervals. If you find you have started too fast, slow down for the next couple of intervals until you can establish a good pace for them. The most effective way to do this workout is to be able to hold close to the same pace for the 10 \u2018on\u2019 intervals (while getting more tired by the end, of course \u2013 you should feel very glad to be finished with the last interval, but not be totally wiped out).</p> <p>42 min [endurance intervals] Row 4 sets of 30/60/90/60/30s \u2018on\u2019 with the same amount of time \u2018off\u2019. The \u2018on\u2019 intervals should be at 80-85% pressure, the \u2018off\u2019 intervals at ~60%. Row 2 minutes easy between each set. Make sure you are fully warmed up before starting this workout. If you row harder during the \u2018on\u2019 segments and easier during the \u2018off\u2019 segments this is more of a speed interval workout. If you row slightly easier on the \u2018on\u2019 intervals, and keep the \u2018off\u2019 intervals firm, this is more of an endurance interval workout. The second part of this week\u2019s workout is a series of ~4 or 5 30-second \u2018sprints\u2019 during which you try to establish a pace by 15s, then try to nudge your split down a few seconds for the last 15 seconds. There will be plenty of rest between these intervals. The goal is to try to break through both mental and physical barriers. ~8-10 min.</p> <p>36 min [endurance intervals, speed] Row 6 sets of 3min on/1 min off/ 90s on/ 30s off. The 3 min intervals should be at ~ 24-26spm, the 90s intervals at 26-30spm.</p> <p>36 min [rating changes] Row 5 sets of 1 minute intervals with ratings 16/20/18/22/20/24; 18/22/20/24/22/26; 20/24/22/26/24/28; 22/26/24/28/26/30; 24/28/26/30/28/32spm. \u00a0Row easy for 1 minute between sets. The goal is to control the rating changes and to hit the target as quickly and efficiently as possible. Note that both big and small changes (4 and 2spm) are incorporated into this workout. Don\u2019t hesitate to back off to lower ratings if you have difficulty with the higher ones in the last 2 sets.</p> <p>35\u00a0min [endurance pyramid] Row a time pyramid of 1/2/3/4/5/4/3/2/1 minutes \u2018on\u2019 with 1 minute \u2018off\u2019 after each. The \u2018on\u2019 segments should be done at ~75-80%; row easy on the \u2018off\u2019 minutes. If this workout is done properly you should be just about able to maintain pressure and form in the last minute \u2018on\u2019.</p> <p>40 min [strength, endurance] Row 40 minutes at LOW rating (~18) and full pressure.</p> <p>40 min [aerobic (threshold)] Row 3 x 12 min at close to head race pressure, increase rating for each piece from 20 to 22 to 24. Row at half pressure for 2 minutes between pieces.</p> <p>40 min [aerobic] Row 40 minutes at a steady pace. Row two power tens \u2013\u00a0at 4 and 9 minutes into each 10 minute segment.</p> <p>40 min [power, speed] Row four sets of 4 x [1 min on/ 1 min off] at ratings of ~24, 26, 28 and 30 (or higher or lower if you prefer) for the first, second, third and fourth sets respectively. The object is to control your stroke and keep the stroke length as you move to the higher ratings. Row at paddle pressure for two minutes between each set.</p> <p>40 min [aerobic] Row five 6-minute pieces divided into 2 minutes each at 18/22/18 followed by 20/24/20, 22/26/22, 20/24/20 and 18/22/18 strokes per minute (spm). The level of effort should be at slightly more than conversational aerobic pace (65-70%) at the lowest rating, increasing to about \u00be pressure (75%) at the highest rating.\u00a0 Do 1 minute of easier rowing (half pressure) after each piece. Finish the workout with 6-8 \u2018tens\u2019 (no higher than 28 spm) before cooling down.</p> <p>40 min [speed, endurance intervals] Row two sets of 5 x [1min \u2018on\u2019/ 2 min \u2018off\u2019]. The \u2018on\u2019 intervals should be at 85-90%, the \u2018off\u2019 intervals at ~50-60% (almost full recovery). Row an extra 2 minutes easy between sets.</p> <p>42 min [anaerobic (threshold)] Row 7 X [3 min @ 22 spm, 2 min @ 25, 1 min @ 28]. Row at challenging intensity, alternating your stroke rate as follows: row 3 minutes @ 22 strokes per minute (spm), 2 minutes @ 25 spm, and 1 minute @ 28 spm, then back to 3 minutes @ 22 spm. Continue to follow this pattern until you have completed 7 cycles for a total work time of 42 minutes.</p> <p>42 min [aerobic (base)] Row six 5-minute pieces at about 65-70% effort (moderate \u2018conversational\u2019). Alternate ratings between 18 and 20, 20 and 22, or 22 and 24spm. At the end of each 5 minute piece row another 30sec at slightly higher intensity (but still &lt; 80%) followed by 1\u00bd\u00a0 minutes at a slightly easier pace (55-65%) before starting the next 5 minute segment.</p> <p>45 min [aerobic] Row 3 x 15 minutes, with each 15 minute piece broken up into 5 minute segments at 18/22/26 spm.</p> <p>46 min [interval pyramid] Row 5 minutes steady state, then row a pyramid of 10/20/30/40/50/40/30/20/10 strokes \u2018on\u2019 with 20 strokes \u2018off\u2019 between each set of \u2018on\u2019 strokes. Repeat the whole workout including the 5 minutes of steady state rowing.</p>"},{"location":"pages/home/vehicles/xterra/build-list/","title":"Build List for My Xterra","text":"<p>This page documents the build of my 2009 Nissan Xterra.</p>"},{"location":"pages/home/vehicles/xterra/build-list/#build-list","title":"Build List","text":""},{"location":"pages/home/vehicles/xterra/build-list/#wheels-tyres","title":"Wheels &amp; Tyres","text":"CurrentPreviousOriginal <ul> <li><code>Tires</code> - Falken Rubitrek A/T LT285/75R16 126R E.</li> <li><code>Wheels</code> - Stock 16\" 30mm offset 114.3mm bolt pattern.</li> <li><code>Spares</code> - todo.</li> </ul> <ul> <li><code>Tires</code> - Toyo Open Country M/T LT285/75R16.</li> <li><code>Wheels</code> - Stock 16\" 30mm offset 114.3mm bolt pattern.</li> <li><code>Spares</code> - todo.</li> </ul> <ul> <li><code>Tires</code> - Yokohama Geolander A/T P265/70R16.</li> <li><code>Wheels</code> - Stock 16\" 30mm offset 114.3mm bolt pattern.</li> <li><code>Spares</code> - todo.</li> </ul>"},{"location":"pages/home/vehicles/xterra/build-list/#suspension-chassis","title":"Suspension &amp; Chassis","text":"Current <ul> <li><code>Lift</code> - 2.5\" (todo: details).</li> <li><code>Shocks</code> - Bilstein 4600.</li> <li><code>Control Arms</code> - OEM replacement.</li> <li><code>Sway Bars</code> - OEM front, removed rear.</li> </ul>"},{"location":"pages/home/vehicles/xterra/build-list/#drivetrain-powertrain","title":"Drivetrain &amp; Powertrain","text":"Current <ul> <li><code>Engine</code> - todo</li> <li><code>Transmission</code> - todo</li> <li><code>Transfer Case</code> - todo</li> <li><code>Differential (front)</code> - todo</li> <li><code>Differential (rear)</code> - todo </li> <li><code>Lockers</code> - todo </li> </ul>"},{"location":"pages/home/vehicles/xterra/build-list/#bumpers-protection","title":"Bumpers &amp; Protection","text":"Current <ul> <li><code>Front Bumper</code> - Coastal Offroad.</li> <li><code>Rear Bumper</code> - Hardcore Offroad.</li> <li><code>Skid Plates</code> - todo.</li> <li><code>Rock Sliders</code> - todo: Hardcore Offroad looks nice and cheap.</li> </ul>"},{"location":"pages/home/vehicles/xterra/build-list/#recovery","title":"Recovery","text":"Current <ul> <li><code>Winch</code> - todo.</li> </ul>"},{"location":"pages/home/vehicles/xterra/build-list/#electrical","title":"Electrical","text":"Current <ul> <li>todo</li> </ul>"},{"location":"pages/home/vehicles/xterra/build-list/#gear","title":"Gear","text":"Current <ul> <li><code>Tent</code> - Front Runner RTT (now Dometic?).</li> </ul>"},{"location":"pages/home/vehicles/xterra/gallery/","title":"Gallery","text":"<p>Warning</p> <p>not implemented yet</p> <p></p>"},{"location":"pages/home/vehicles/xterra/gallery/#bumper","title":"Bumper","text":""},{"location":"projects/fathommap/","title":"FathomMap","text":"<p> source code</p>"},{"location":"projects/fathommap/#overview","title":"Overview","text":"<p>Underwater Content-Based Image Retrieval.</p> <pre><code>graph LR\n\n    Q[Query]\n    D[Descriptor]\n    S[Searcher]\n    DB[(Database)]\n    R[Results]\n\n    Q --&gt; |image|D\n    D --&gt; |feature vector|S\n    S --&gt; |feature vector|DB --&gt; |images|S\n    S --&gt; |images|R</code></pre> <p>FathomMap is a Content-Based Image Retrieval (CBIR) framework designed to evaluate and compare multiple retrieval pipelines for underwater imagery. The project explores how different Descriptors (feature extractors) and Searchers affect retrieval accuracy and efficiency, with a particular focus on leveraging learned representations from the FathomView UNet model - a previous project for enhancing underwater images.</p> <p>Given a query image, FathomMap first extracts a compact feature vector using an image Descriptor (e.g., UNet-based encoder, pretrained ResNet18, HE). These feature vectors are then compared against a database of precomputed desciptors using an image Searcher (e.g., linear search or indexing with FAISS and inverted K-Means clustering).</p> <p>A key motivation for this project is to assess whether a UNet trained for underwater image enhancement can also serve as an effective semantic descriptor for retrieval, compared against more conventional CNN-based or classical approaches.</p> <p>The overall system architecture is illustrated above, highlighting the separation of feature extraction, image/feature storage and similarity search.</p>"},{"location":"projects/fathommap/#results","title":"Results","text":"<p>The following MSE and MS-SSIM metrics are explained more in my complementary project - FathomView - where they were used to compose a loss function for training a custom UNet model to enhance underwater images.</p> <p></p> <p>This figure evaluates the accuracy of selected image descriptors. FathomMap's UNet Descriptor outperforms other approaches. The plot on the right shows the degree of similarity between the query image and the returned images. The plots comparing MSE and MS-SSIM reinforce this evaluation by showing the model's attention to both pixel-level (MSE) and structural (MS-SSIM) similarities.</p> <p></p> <p>This figure evaluates the efficiency of selected image searchers/index strategies. The key take-away here is that FAISS outperforms inverted K-Means for UNet descriptors because the enhancement-trained UNet features are highly correlated and poorly clustered, while ResNet's features are semantically separable and benefit more from clister-based pruning. This illustrates the importance in separating the image descriptors and searchers for evaluation.</p>"},{"location":"projects/fathommap/#key-components","title":"Key Components","text":""},{"location":"projects/fathommap/#descriptors","title":"Descriptors","text":"<p><code>class fathommap.descriptors.Base(ABC)</code> <code>class fathommap.descriptors.UNet(Base)</code> <code>class fathommap.descriptors.ResNet(Base)</code> <code>class fathommap.descriptors.Histogram(Base)</code></p> Descriptor APIUNetResNet 18Histogram <pre><code># Abstract base class for Content-Based Image Retrieval (CBIR) descriptors.\n\nclass BaseDescriptor(ABC):\n\n    @abstractmethod\n    def describe(self, image: np.ndarray) -&gt; np.ndarray:\n        # Returns features extracted from the given `image`.\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        # Returns the name of the descriptor.\n</code></pre> <pre><code>class UNetDescriptor(BaseDescriptor):\n\n    def __init__(self, model_path):\n        unet = Unet()\n        unet.load_state_dict(torch.load(model_path))\n        unet.eval()\n        transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.ToTensor(),\n            transforms.Resize((224, 224)),\n            transforms.Normalize(\n                mean=[0.5, 0.5, 0.5],\n                std=[0.5, 0.5, 0.5],\n            )\n        ])\n\n        self.unet = unet\n        self.transform = transform\n\n    def describe(self, image: np.ndarray) -&gt; np.ndarray:\n\n        _image = copy.deepcopy(image)\n        _image = cv2.cvtColor(_image, cv2.COLOR_BGR2RGB)\n        _image = self.transform(_image)\n        _image = _image.unsqueeze(0)\n\n        with torch.no_grad():\n            features = self.unet(_image)\n\n        features = features.flatten().numpy()\n\n        return features\n</code></pre> <pre><code>class ResNetDescriptor(BaseDescriptor):\n\n    def __init__(self):\n        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n        resnet.eval()\n        transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(  # normalize with ImageNet values\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            ),\n        ])\n\n        self.resnet = resnet\n        self.transform = transform\n\n    def describe(self, image: np.ndarray) -&gt; np.ndarray:\n\n        _image = copy.deepcopy(image)\n        _image = cv2.cvtColor(_image, cv2.COLOR_BGR2RGB)\n        _image = self.transform(_image)\n        _image = _image.unsqueeze(0)\n\n        with torch.no_grad():\n            features = self.resnet(_image)\n\n        features = features.flatten().numpy()\n\n        return features\n</code></pre> <pre><code>class HistDescriptor(BaseDescriptor):\n\n    def __init__(self, bins):\n        self.bins = bins\n\n    def describe(self, image: np.ndarray) -&gt; np.ndarray:\n\n        _image = copy.deepcopy(image)\n        _image = cv2.cvtColor(_image, cv2.COLOR_BGR2HSV)\n        features = []\n\n        # Divide the image into four segments and an elliptical center.\n        (h, w) = _image.shape[:2]\n        (cx, cy) = (int(w * 0.5), int(h * 0.5))  # center point\n        segments = [\n            (0, cx, 0, cy),  # top left\n            (cx, w, 0, cy),  # top right\n            (cx, w, cy, h),  # bottom right\n            (0, cx, cy, h),  # bottom left\n        ]\n        (ax, ay) = (int(w * 0.75) // 2, int(h * 0.75) // 2)\n        ellipse_mask = np.zeros(_image.shape[:2], dtype=\"uint8\")\n        cv2.ellipse(ellipse_mask, (cx, cy), (ax, ay), 0, 0, 360, 255, -1)\n\n        # Compute Histogram for Corner Segments.\n        for (x0, xf, y0, yf) in segments:\n            corner_mask = np.zeros(_image.shape[:2], dtype=\"uint8\")\n            cv2.rectangle(corner_mask, (x0, y0), (xf, yf), 255, -1)\n            corner_mask = cv2.subtract(corner_mask, ellipse_mask)\n\n            hist = self.histogram(_image, corner_mask)\n            features.extend(hist)\n\n        # Compute Histogram for Elliptical Center.\n        hist = self.histogram(_image, ellipse_mask)\n        features.extend(hist)\n        features = np.array(features)\n\n        return features\n\n    def histogram(self, image, mask):\n\n        hist = cv2.calcHist(\n            images=[image],\n            channels=[0, 1, 2],\n            mask=mask,\n            histSize=self.bins,\n            ranges=[0, 180, 0, 256, 0, 256],\n        )\n\n        # Normalize the Histogram According to OpenCV Version.\n        if imutils.is_cv2():  # OpenCV 2.4\n            hist = cv2.normalize(hist).flatten()\n        else:  # OpenCV 3+\n            hist = cv2.normalize(hist, hist).flatten()\n\n        return hist\n</code></pre> <p>Descriptors are responsible for converting an input image into a fixed-length feature vector that captures visual or semantic information suitable for similarity search.</p>"},{"location":"projects/fathommap/#unet","title":"UNet","text":"<p>Uses a UNet model trained for underwater image enhancement as a learned feature extractor. The model output is flattened to form a high-dimensional descriptor, capturing scene structure and color characteristics tailored to underwater imagery.</p>"},{"location":"projects/fathommap/#resnet","title":"ResNet","text":"<p>Leverages a pretrained ResNet-18 model to extract deep semantic features learned from large-scale natural image datasets. Serves as a strong general-purpose baseline for CNN-based image retrieval.</p>"},{"location":"projects/fathommap/#histogram","title":"Histogram","text":"<p>Computes spatially-aware HSV color histograms over image regions. This hand-crafted descriptor provides a lightweight, interpretable baseline that emphasizes global color distribution rather than semantic content.</p>"},{"location":"projects/fathommap/#searchers","title":"Searchers","text":"<p><code>class fathommap.searchers.Base(ABC)</code> <code>class fathommap.searchers.Brute(Base)</code> <code>class fathommap.searchers.FaissFlatIp(Base)</code> <code>class fathommap.searchers.InvertedKMeans(Base)</code></p> Searcher APIBrute ForceFAISS FLAT IPInverted KMeans Clustering <pre><code># Abstract base class for Content-Based Image Retrieval (CBIR) searchers.\n\nclass BaseSearcher(ABC):\n\n    def __init__(self, similarity_fn: Callable[[np.ndarray, np.ndarray], float]):\n        self.similarity_fn = similarity_fn\n\n    @abstractmethod\n    def search(\n        self,\n        query_vector: np.ndarray,\n        limit: int = 10\n    ) -&gt; list[tuple[str, float]]:\n        # Returns the top `limit` most similar items to the given `query_vector`.\n\n    @abstractmethod\n    def build_index(self, features_path: str) -&gt; None:\n        # Builds the internal data structure required for similarity search.\n</code></pre> <pre><code>class BruteForceSearcher:\n\n    def __init__(self, similarity_fn):\n        self.similarity_fn = similarity_fn\n        self.labels = None\n        self.features = None\n\n    def search(\n        self,\n        query_vector: np.ndarray,\n        limit: int = 10\n    ) -&gt; list[tuple[str, float]]:\n        # Compute similarities without FAISS Index.\n        results = [  # (label, similarity)\n            (label, self.similarity_fn(features, query_vector))\n            for label, features in zip(self.labels, self.features)\n        ]\n        # Reverse Sort Results by Similarity Value.\n        results.sort(key=lambda item: item[1], reverse=True)\n\n        return results[:limit]\n\n    def build_index(self, feature_path):\n        self.labels = []\n        self.features = []\n        with open(feature_path, \"r\", encoding=\"utf-8\") as f:\n            reader = csv.reader(f)\n            _ = next(reader)  # header\n            for label, *features in reader:\n                self.labels.append(label)\n                self.features.append([float(x) for x in features])\n</code></pre> <pre><code>class FaissFlatIPSearcher:\n\n    def __init__(self, similarity_fn=None):\n        self.similarity_fn = similarity_fn\n        self.labels = None\n        self.index = None\n\n    def search(\n        self,\n        query_vector: np.ndarray,\n        limit: int = 10,\n    ) -&gt; list[tuple[str, float]]:\n        query_vector = np.array(query_vector).astype(\"float32\").reshape(1, -1)\n        faiss.normalize_L2(query_vector)\n\n        distances, indices = self.index.search(\n            query_vector,\n            k=limit or len(self.labels),\n        )\n\n        results = [\n            (self.labels[i], distances[0][rank])\n            for rank, i in enumerate(indices[0])\n        ]\n\n        return results[:limit]\n\n    def build_index(self, feature_path: str) -&gt; None:\n        labels = []\n        features = []\n        with open(feature_path, \"r\", encoding=\"utf-8\") as f:\n            reader = csv.reader(f)\n            _ = next(reader)  # header\n            for label, *_features in reader:\n                labels.append(label)\n                features.append([float(x) for x in _features])\n\n        self.labels = labels  # todo: this won't work for loading saved index\n        features = np.array(features).astype(\"float32\")\n        faiss.normalize_L2(features)\n        self.index = faiss.IndexFlatIP(features.shape[1])\n        self.index.add(features)\n</code></pre> <pre><code>from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import normalize\n\n\nclass InvertedSearcher:\n\n    def __init__(self, similarity_fn=None, n_clusters: int = 10):\n        self.similarity_fn = similarity_fn\n        self.n_clusters = n_clusters\n        self.index = defaultdict(list)  # cluster_id -&gt; [(label, vector), ...]\n        self.kmeans = None\n        self.labels = None\n\n    def search(\n        self,\n        query_vector: np.ndarray,\n        limit: int = 10,\n        probe: int = 3,\n    ) -&gt; list[tuple[str, float]]:\n        query_vector = normalize(query_vector.reshape(1, -1), norm=\"l2\")[0]\n\n        # Find `probe` nearest centroids.\n        centroids = normalize(self.kmeans.cluster_centers_, norm=\"l2\")\n        similarities = centroids @ query_vector.T  # cosine similarity\n        top_cluster_ids = np.argsort(similarities)[-probe:][::-1]\n\n        # Search within the selected clusters.\n        results = []\n        for cid in top_cluster_ids:\n            for label, vec in self.index[cid]:\n                sim = np.dot(query_vector, vec)  # cosine similarity\n                results.append((label, sim))\n\n        results.sort(key=lambda x: x[1], reverse=True)\n        return results[:limit]\n\n    def build_index(self, feature_path: str) -&gt; None:\n        labels = []\n        features = []\n        with open(feature_path, \"r\", encoding=\"utf-8\") as f:\n            reader = csv.reader(f)\n            _ = next(reader)  # header\n            for label, *_features in reader:\n                labels.append(label)\n                features.append([float(x) for x in _features])\n\n        self.labels = labels\n        features = np.array(features).astype(\"float32\")\n        features = normalize(features, norm=\"l2\")  # normalize for cosine\n\n        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42)\n        cluster_ids = self.kmeans.fit_predict(features)\n\n        self.index = defaultdict(list)\n        for label, vec, cid in zip(labels, features, cluster_ids):\n            self.index[cid].append((label, vec))\n</code></pre> <p>Searchers operate over a collection of stored feature vectors and return the most similar items to a given query vector.</p>"},{"location":"projects/fathommap/#brute-force","title":"Brute Force","text":"<p>Computes similarity between the query vector and every feature vector in the database. While simple and exact, this approach does not scale well to large datasets.</p>"},{"location":"projects/fathommap/#faiss-flat-inner-product","title":"FAISS Flat Inner Product","text":"<p>Uses Facebook AI Similarity Search (FAISS) with L2-normalized vectors and inner-product similarity for efficient exact nearest-neighbor search. Provides significantly faster queries while maintaining exact results.</p>"},{"location":"projects/fathommap/#inverted-k-means-clustering","title":"Inverted K-Means Clustering","text":"<p>Partitions feature vectors into clusters using K-Means and restricts search to the most relevant clusters at query time. This approximate approach improves scalability by reducing the number of comparisons required per query.</p>"},{"location":"projects/fathomview/","title":"FathomView","text":"<p> source code</p>"},{"location":"projects/fathomview/#overview","title":"Overview","text":"<p>Underwater Image Enhancement using Deep Convolutional Neural Networks.</p> <pre><code>flowchart LR\n    input[\"Input Image&lt;br&gt;&lt;br&gt;&lt;img src='/assets/code/fathomview/8-input.jpg' width='60'/&gt;\"]\n    fv[FathomView]\n    output[\"Enhanced Image&lt;br&gt;&lt;br&gt;&lt;img src='/assets/code/fathomview/8-label.jpg' width='60'/&gt;\"]\n\n    %% Subgraph inside FathomView\n    subgraph fv [FathomView Pipeline]\n        preprocess[\"Preprocessing\"]\n        model[\"UNet Model&lt;br&gt;&lt;br&gt;&lt;img src='/assets/code/unet-diagram.png' width='60'/&gt;\"]\n        postprocess[\"Postprocessing\"]\n        preprocess --&gt; model --&gt; postprocess\n    end\n\n    input --&gt; fv --&gt; output</code></pre> <p>FathomView implements a convolutional UNet-style encoder-decoder architecture to enhance underwater images by correcting color distortion, low contrast, and blurriness caused by light absorption and scattering in underwater environments.</p> <p>The model is trained using supervised learning on paired datasets of underwater images and their corresponding enhanced versions, learning a direct image-to-image mapping to restore visual quality. Spatial detail is preserved through skip connections between encoder and decoder layers.</p> <p>FathomView's U-Net architecture is compared against other models including traditional baselines and other deep learning approaches, demonstrating superior performance in terms of PSNR and SSIM metrics on benchmark underwater image datasets.</p>"},{"location":"projects/fathomview/#results","title":"Results","text":"<p>Comparing FathomView's output across datasets. Left: Trained on EUVP dataset. Right: Trained on LSUI dataset.</p> Metric FathomView UCM HE MSRCR Fusion UDCP IBLA UGAN WaterGAN <code>MSE</code> 0.002 0.029 0.045 0.059 0.027 0.072 0.058 0.026 0.014 <code>PSNR</code> 30.31 20.68 18.315 13.25 23.13 17.37 19.10 20.63 20.25 <code>SSIM</code> 0.966 0.869 0.845 0.580 0.933 0.847 0.832 0.779 0.842 <p>FathomView achieves the best performance in both PSNR and SSIM metrics, indicating better visual quality and detail preservation in enhanced images.</p>"},{"location":"projects/fathomview/#key-components","title":"Key Components","text":""},{"location":"projects/fathomview/#unet-architecture","title":"UNet Architecture","text":"<p><code>class torch.nn.Module</code> <code>class fathomview.models.UNet(nn.Module)</code></p> UNet.__init__()UNet.forward()conv_block() <pre><code>def __init__(self, in_channels: int, out_channels: int, init_features: int):\n\n    x = init_features\n    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    self.e1 = conv_block(in_channels, x)              # 3 =&gt; 32\n    self.e2 = conv_block(x, x * 2)                    # 32 =&gt; 64\n    self.e3 = conv_block(x * 2, x * 4, dropout=True)  # 64 =&gt; 128\n\n    self.b = conv_block(x * 4, x * 8, dropout=True)   # 128 =&gt; 256\n\n    self.uc3 = nn.ConvTranspose2d(x * 8, x * 4, kernel_size=2, stride=2)  # 256 =&gt; 128\n    self.uc2 = nn.ConvTranspose2d(x * 4, x * 2, kernel_size=2, stride=2)  # 128 =&gt; 64\n    self.uc1 = nn.ConvTranspose2d(x * 2, x, kernel_size=2, stride=2)      # 64 =&gt; 32\n    self.d3 = conv_block(x * 8, x * 4)\n    self.d2 = conv_block(x * 4, x * 2)\n    self.d1 = conv_block(x * 2, x)\n\n    self.conv = nn.Conv2d(x, out_channels, kernel_size=1)\n</code></pre> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n\n    # Encoder (downsampling with max pooling)\n    enc1 = self.encoder1(x)\n    enc2 = self.encoder2(self.pool(enc1))\n    enc3 = self.encoder3(self.pool(enc2))\n\n    # Bottleneck\n    bottleneck = self.bottleneck(self.pool(enc3))\n\n    # Decoder (upsampling with transposed convolutions)\n    dec3 = self.upconv3(bottleneck)\n    dec3 = torch.cat((dec3, enc3), dim=1)  # skip connection\n    dec3 = self.decoder3(dec3)\n    dec2 = self.upconv2(dec3)\n    dec2 = torch.cat((dec2, enc2), dim=1)  # skip connection\n    dec2 = self.decoder2(dec2)\n    dec1 = self.upconv1(dec2)\n    dec1 = torch.cat((dec1, enc1), dim=1)  # skip connection\n    dec1 = self.decoder1(dec1)\n\n    return self.conv(dec1)\n</code></pre> <pre><code>def conv_block(in_channels, features, dropout=False):\n\n    layers = [\n        nn.Conv2d(in_channels, features, kernel_size=3, padding=1, bias=False),\n\n        nn.ReLU(inplace=True),\n\n        nn.Conv2d(features, features, kernel_size=3, padding=1, bias=False),\n\n        *( [nn.Dropout2d(0.3)] if dropout else []),  # optional dropout layer\n\n        nn.ReLU(inplace=True),\n    ]\n\n    return nn.Sequential(*layers)\n</code></pre> <p>The UNet class implements a standard encoder-decoder architecture with skip connections, originally introduced in (Ronneberger, Fischer, and Brox, 2015) for biomedical image segmentation and adapted here for underwater image enhancement.</p> <p>It features symmetric downsampling and upsampling paths, feature reuse via concatenated skip connections, and optional dropout for regularization in deeper layers.</p> <p> The overall UNet-style architecture of this work. </p> <p>At a high level, the forward pass is responsible for defining a pass through the network with the following pipeline:</p> <ol> <li>Encoder: Downsample input image via max pooling, reducing spatial dimensions while increasing feature depth.</li> <li>Bottleneck: Highest-level feature representation; dropout applied to reduce overfitting.</li> <li>Decoder: Upsampling via transposed convolutions to restore original spatial resolution; skip connections concatenate encoder features.</li> </ol> <p>While the above figure demonstrates this pass through in context with the various operations performed at each layer, the following diagram illustrates a high level overview of the encode-decode process with skip-connections.</p> <pre><code>graph TB\n    E1[Encoder 1]\n    E2[Encoder 2]\n    E3[Encoder 3]\n    B[Bottleneck]\n    D3[Decoder 3]\n    D2[Decoder 2]\n    D1[Decoder 1]\n\n    %% Connections\n    B --&gt; D3\n    E1 --&gt; E2\n    E2 --&gt; E3\n    E3 --&gt; B\n    D3 --&gt; D2\n    D2 --&gt; D1\n\n    %% Skip connections\n    E3 -.-&gt; D3\n    E2 -.-&gt; D2\n    E1 -.-&gt; D1</code></pre>"},{"location":"projects/fathomview/#objective-loss-function","title":"Objective (Loss) Function","text":"<p>L1 Loss - Mean Absolute Error (MSE)</p> \\[ L_{L1}(x, y) = \\frac{1}{N}\\sum_{i=0}^{N}\\left|x_i - y_i\\right| \\] <ul> <li>Measures the average absolute difference between predicted (x) and ground truth (y) pixels.</li> <li>Encourages high PSNR and preserves overall color and luminance.</li> <li>Limitation: ignores structural and perceptual features of images.</li> </ul> <p>SSIM Loss - Structural SIMilarity</p> \\[ L_{SSIM}(x, y) = 1 - SSIM(x, y) \\] <p>where</p> \\[ SSIM(x, y) = l(x, y)^\\alpha * c(x, y)^\\beta * s(x, y)^\\gamma \\] <ul> <li><code>l(x, y)</code> - luminance comparison (mean intensity)</li> <li><code>c(x, y)</code> - contrast comparison (standard deviation)</li> <li><code>s(x, y)</code> - structure comparison (correlation between patterns)</li> <li>Exploits pixel inter-dependencies to evaluate structural similarity.</li> </ul> <p>MS SSIM Loss - Multi-Scaled SSIM</p> \\[ L_{MS-SSIM}(x, y) = 1 - \\prod_{j=1}^{M}SSIM_j(x, y) \\] <ul> <li>Computes SSIM over multiple scales (j) to capture both local textures and global structures.</li> <li>More sensitive to perceptual differences across different resolutions.</li> </ul> <p>FathomView Loss</p> \\[ L_{FathomView}(x, y) = L_{L1}(x, y) + L_{MS-SSIM}(x, y) \\] <ul> <li>Combines pixel-level accuracy (L1) with perceptual and structural similarity (MS-SSIM).</li> <li>Encourages enhanced images to maintain brightness, color, and structure.</li> </ul>"},{"location":"projects/lobo-shell/","title":"Lobo Shell","text":"<p> source code</p>"},{"location":"projects/lobo-shell/#overview","title":"Overview","text":"<p>Lobo Shell is a simple Unix shell implemented in C that demonstrates the following:</p> <ul> <li>Interacting with the operating system via system calls.</li> <li>Tokenization and parsing of command-line input.</li> <li>Command pipelines.</li> <li>Redirection operators (e.g., <code>&lt;</code>, <code>&gt;</code>, <code>&gt;&gt;</code>).</li> <li>Interactive command loop.</li> </ul> <pre><code>graph LR\n  Input@{ shape: f-circ, label: \"Junction\" }\n  Lexer[Lexer]\n  Parser[Parser]\n  Executor[Executor]\n  Output@{ shape: f-circ, label: \"Junction\" }\n\n  Input --&gt; |text| Lexer --&gt; |tokens| Parser --&gt; |commands| Executor --&gt; |output| Output</code></pre>"},{"location":"projects/lobo-shell/#key-components","title":"Key Components","text":""},{"location":"projects/lobo-shell/#types","title":"Types","text":"TokenTokenStreamCommandPipeline <pre><code>typedef struct Token {\n    TokenType type;\n    char *text; // lexeme\n} Token;\n</code></pre> Example <pre><code>Token pipe          = { T_PIPE: \"|\" };\nToken redir_in      = { T_REDIR_IN: \"&lt;\" };\nToken redir_out     = { T_REDIR_OUT: \"&gt;\" };\nToken redir_append  = { T_REDIR_APPEND: \"&gt;&gt;\" };\nToken word          = { T_WORD: \"everything_else\" };\n</code></pre> <pre><code>typedef struct TokenStream {\n    Token *tokens;\n    int size;\n} TokenStream;\n</code></pre> Example <pre><code>input = \"grep foo &lt; in.txt | sort &gt; out.txt | uniq &gt;&gt; append.txt\";\n\ntokens = {\n    { T_WORD: \"grep\" },\n    { T_WORD: \"foo\" },\n    { T_REDIR_IN: \"&lt;\" },\n    { T_WORD: \"in.txt\" },\n\n    { T_PIPE: \"|\" },\n\n    { T_WORD: \"sort\" },\n    { T_REDIR_OUT: \"&gt;\" },\n    { T_WORD: \"out.txt\" },\n\n    { T_PIPE: \"|\" },\n\n    { T_WORD: \"uniq\" },\n    { T_REDIR_APPEND: \"&gt;&gt;\" },\n    { T_WORD: \"append.txt\" }\n};\n</code></pre> <p>Warning</p> <p>I am refactoring for a new Command struct.</p> <pre><code>typedef struct Command {\n    char **argv;\n    char *infile;\n    char *outfile;\n    bool append;\n    bool is_first;\n    bool is_last;\n} Command;\n</code></pre> Example <pre><code>input = \"grep foo &lt; in.txt | sort &gt; out.txt | uniq &gt;&gt; append.txt\";\n\ncmd_1 = {\n    argv: [ \"grep\", \"foo\", NULL ],\n    infile: \"in.txt\",\n    outfile: NULL,\n    append: false,\n    is_first: true,\n    is_last: false\n};\n\ncmd_2 = {\n    argv: [ \"sort\", NULL ],\n    infile: NULL,\n    outfile: \"out.txt\",\n    append: false,\n    is_first: false,\n    is_last: false\n};\n\ncmd_3 = {\n    argv: [ \"uniq\", NULL ],\n    infile: NULL,\n    outfile: \"append.txt\",\n    append: true,\n    is_first: false,\n    is_last: true\n};\n</code></pre> <pre><code>typedef struct Pipeline {\n    Command *cmds;\n    int size;\n} Pipeline;\n</code></pre> Example <pre><code>input = \"grep foo &lt; in.txt | sort &gt; out.txt | uniq &gt;&gt; append.txt\";\n\npipeline = {\n    cmds: [ cmd_1, cmd_2, cmd_3 ],\n    size: 3\n};\n</code></pre>"},{"location":"projects/lobo-shell/#functions","title":"Functions","text":"lexerparserexecutor <pre><code>TokenStream lexer(char *line) {\n    // &lt;line&gt; --&gt; &lt;tokens&gt;\n}\n</code></pre> <pre><code>Pipeline parse_tokens(TokenStream ts) {\n    // &lt;tokens&gt; --&gt; &lt;commands&gt;\n}\n</code></pre> <pre><code>void execute_pipeline(Pipeline pl) {\n    // execute &lt;commands&gt;\n}\n</code></pre>"},{"location":"projects/neuroloop/","title":"NeuroLoop","text":"<p> source code</p>"},{"location":"projects/neuroloop/#overview","title":"Overview","text":"<p>Closed-Loop Deep-Brain Stimulation for Controlling Synchronization of Spiking Neurons.</p> <p> The overall architecture of this work. </p> <p>NeuroLoop implements a closed-loop deep brain stimulation (cl-DBS) system to modulate neural synchronization in a computational model of the brain, optimizing the open-loop regime described in (Schmalz &amp; Kumar, 2019) by implementing a Reinforcement-Learning (RL) driven feedback controller that adjusts stimulation parameters based on real-time measurements of network synchronization.</p> <p>Pathological neural synchronization is a fundamental characteristic of several neurological and neuropsychiatric disorders, including Parkinson\u2019s disease, epilepsy, and major depressive disorder, where excessive coupling between neural populations can directly contribute to debilitating symptoms such as tremor, seizures, and cognitive impairment. Deep brain stimulation (DBS) is an established therapy for modulating abnormal activity, but current clinical systems often operate in an open-loop manner, delivering stimulation continuously without adapting to the brain\u2019s evolving state.</p>"},{"location":"projects/neuroloop/#results","title":"Results","text":"<p>Training Loss </p> <p>Episode Reward Mean </p> <p>Neural Synchronization </p> <p>Mean Synaptic Weight </p> <p>Neuron Spike Times (pre-stim) </p> <p>Neuron Spike Times (post-stim) </p> <p>Successfully learns to reduce neural synchronization and forces a spiking pattern.</p>"},{"location":"projects/neuroloop/#system-model","title":"System Model","text":"<p>This section describes the neural system model, control objectives, and reinforcement learning formulation underlying the NeuroLoop framework.</p>"},{"location":"projects/neuroloop/#neural-system-model","title":"Neural System Model","text":"<p>Excitatory-Inhibitory (EI) Network Model</p> <p>The neural system in this work is an Excitatory-Inihibitory (EI) Network of Leaky-Integrate-and-Fire (LIF) Neurons with Spike-Timing-Dependent Plasticity. The following LIF model (Brunel &amp; Hansel, 2006; Vlachos et al., 2016) describes a single neuron's dynamics in the EI network:</p> \\[ \\tau_m \\frac{d v_i(t)}{d t} =   - v_i(t)   + Z_i(t)   + \\mu_i   + \\sigma_i \\sqrt{\\tau_m}\\,\\chi(t)   + V_i(t) \\] \\[ Z_i(t) = \\frac{J_\\text{ij}}{C_\\text{ij}} S_\\text{ij}(t) \\] \\[ \\tau_d \\frac{d S_\\text{ij}(t)}{d t} = - S_\\text{ij}(t) + X_\\text{ij}(t) \\] \\[ \\tau_r \\frac{d X_\\text{ij}(t)}{d t} = - X_\\text{ij}(t) + W_\\text{ij}(t) \\delta(t - t_\\text{pre} + t_\\text{delay}) \\] Description of Terms <p>Note: \\(i\\) and \\(j\\) \\(\\epsilon \\{E, I\\}\\), denoting a neuron population.</p> Term Units Description \\(\\tau_m\\) ms Membrane time constant. \\(v_i(t)\\) mV The neuron's membrane potential. \\(Z_i(t)\\) n/a The synaptic input to the neuron population. \\(\\mu_i + \\sigma_i \\sqrt{\\tau_m} \\chi(t)\\) n/a Gaussian distributed baseline current to the neuron. \\(\\chi(t)\\) n/a White noise with a mean of 0 and variance of 1. \\(V_i(s)\\) mV External stimulation to the neuron. \\(J_\\text{ij}\\) mV Synaptic strength between a presynaptic j-neuron and postsynaptic i-neuron. \\(S_\\text{ij}(t)\\) n/a The synaptic function. \\(\\tau_d, \\tau_r\\) ms Decay and rise time constants. \\(X_\\text{ij}\\) n/a Input to the \\(i^\\text{th}\\) population from the $j^\\text{th} population. \\(W_\\text{ij}(t)\\) n/a The weights of each synaptic connection (plastic). \\(\\delta(t - t_\\text{pre} + t_\\text{delay})\\) n/a The Dirac-Delta function modeling synaptic input to a postsynaptic neuron from a presynaptic neuron when the presynaptic neuron fires at time \\(t_\\text{pre}\\) with a synaptics delay of \\(t_\\text{delay}\\). <p>Spike-Timing-Dependent Plasticity (STDP) Model</p> <p>The coupling value of the plastic EI synapse (\\(W_\\text{EI}(t)\\)) is governed by STDP (Song et al., 2000; Ebert et al., 2014), defined as follows:</p> \\[ W_\\text{IE}(t + \\Delta t) = W_\\text{IE}(t) + \\Delta W_\\text{IE}(t) \\] \\[ \\Delta W_\\text{IE}(t) =   \\begin{cases}     \\eta_e \\alpha_\\text{LTP} A_\\text{post}, &amp; \\text{if } t_\\text{pre} - t_\\text{post} &lt; 0 \\\\     \\eta_e \\alpha_\\text{LTD} A_\\text{pre}, &amp; \\text{if } t_\\text{pre} - t_\\text{post} &gt; 0   \\end{cases} \\] \\[ \\tau_\\text{LTP} \\frac{d A_\\text{post}}{d t} = - A_\\text{post} + A_0 \\delta(t - t_\\text{post}) \\] \\[ \\tau_\\text{LTD} \\frac{d A_\\text{pre}}{d t} = - A_\\text{pre} + A_0 \\delta(t - t_\\text{pre}) \\] Description of Terms Term Units Description \\(W_\\text{ij}(t)\\) n/a The weights of each synaptic connection (plastic). \\(\\Delta W_\\text{IE}\\) n/a The change in synaptic weight determined by the spike-time of a presynaptic (\\(t_\\text{pre}\\)) and postsynaptic (\\(t_\\text{post}\\)) neuron. \\(\\eta_e\\) n/a The rate at which the E-to-I synaptic coupling changes. \\(\\alpha_{LTP}, \\alpha_{LTD}\\) n/a The relative contributions of LTP and LTD to \\(\\Delta W_\\text{IE}\\). \\(\\tau_\\text{LTP}, \\tau_\\text{LTD}\\) ms STDP time constants defining the size of the long-term potentiation and long-term depression time window. \\(A_0\\) n/a Fixed jump added to a trace at each spike. \\(A_\\text{pre}\\) n/a Presynaptic spike trace (decaying memory of recent pre spikes). \\(A_\\text{post}\\) n/a Postsynaptic spike trace (decaying memory of recent post spikes)."},{"location":"projects/neuroloop/#control-objective","title":"Control Objective","text":"<p>The primary objective of NeuroLoop is to supress pathological neural synchronization while minimizing the required stimulation intensity to do so. Excessive stimulation amplitude is undesirable due to increased energy consumption and the risk of stimulation-induced side effects. Control is implemented in a closed-loop manner, where stimulation parameters are continuously adjusted based on real-time observations of neural activity.</p> <p>At each control timestep, stimulation amplitude is applied to the neural system according to the FTSTS motif. The resulting neural dynamics are observed, enabling the controller to respond to changes in network state. The control objective and feedback signals are formalized in the following section.</p>"},{"location":"projects/neuroloop/#problem-formulation","title":"Problem Formulation","text":"<p>NeuroLoop is formulated as a Markov Decision Process (MDP) defined by the tuple (\\(S, A, P, R\\)).</p> <p>MDP Terms</p> <p>\\(S\\) - The state space; encodes the feedback signals. \\(A\\) - The action space; encodes the control signals (canonical DBS parameters). \\(P\\) - The transition dynamics. \\(R\\) - The reward function; encodes the control objective.</p>"},{"location":"projects/neuroloop/#environment","title":"Environment","text":"<p>NeuroLoop wraps the neural model described above in a Gymnasium-style environment for developing a Reinforcement Learning (RL) controller as depicted in the figure at the top of this page. This environment structure provides a standard interface to the action space, state space, reward function, and episode configuration for the RL agent to interact with.</p> <pre><code>flowchart LR\n\n    subgraph gym [Gymnasium]\n      nn(Neural Model)\n      env(FTSTSEnv)\n    end\n\n    subgraph controller [Controller Unit]\n      agent(RL Algorithm)\n      features(Feature Extraction)\n    end\n\n    agent -- action --&gt; env\n    env -- reward --&gt; agent\n    env -- state --&gt; features\n    features -- state --&gt; agent\n    env --&gt; nn --&gt; env</code></pre>"},{"location":"projects/neuroloop/#action-space","title":"Action Space","text":"Index Action Unit Range 0 Amplitude mV [10, 200] 1 Frequency Hz [5, 128] 2 Pulse Width \\(\\mu\\)s [50, 500] <p>The agent\u2019s action space consists of a single continuous variable corresponding to the stimulation amplitude applied within the FTSTS framework. The remaining canonical DBS parameters (i.e., frequency and pulse width) are fixed according to the FTSTS motif, allowing the effects of adaptive amplitude modulation to be isolated and analyzed. Actions are represented in a normalized space and linearly rescaled to the biophysically meaningful stimulation range prior to application. For example, the stimulation amplitude is a continuous variable internally normalized to the range [\u22121, 1] and mapped to the suggested range of [10, 200].</p>"},{"location":"projects/neuroloop/#state-space","title":"State Space","text":"Index Observation Unit Range 0 Synchrony n/a [0, 1] 1 Mean Excitatory Membrane Voltage mV [\u221280, 50] 2 Std. Excitatory Membrane Voltage mV [0, 30] 3 Mean Inhibitory Membrane Voltage mV [\u221280, 50] 4 Std. Inhibitory Membrane Voltage mV [0, 30] 5 Mean \\(I \\rightarrow E\\) Synaptic Weight n/a [0, 1] \\[ s_t = (synchrony, \\mu_{V_E}, \\sigma_{V_E}, \\mu_{V_I}, \\sigma_{V_I}, \\mu_{W_\\text{EI}}) \\] <p>At each control timestep, the reinforcement learning agent observes a compact state vector derived from population-level neural statistics. All feedback signals are aggregated over a fixed temporal window and standardized prior to control by subtracting a running mean and dividing by a running standard deviation computed over interaction trajectories. This normalization ensures that each state dimension has approximately zero mean and unit variance, stabalizing learning dynamics across stimulation regimes.</p> <p>The synchrony measurement used here is a per-timestep estimate of voltage-based population synchrony suitable for closed-loop control.</p> \\[ synchrony =   \\sqrt{\\frac     {\\sigma_t(\\bar{v}(t))}     {\\frac{1}{N} \\sum_{i=1}^N \\sigma_t(v_i(t))}   } \\] \\[ \\bar{v}(t) = \\frac{1}{N} \\sum_{i=1}^N v_i(t) \\] <p>where \\(v_i(t)\\) is the membrane potential of neuron \\(i\\) and \\(\\bar{v}(t)\\) is the population mean voltage. Values near zero indicate asynchronous activity while larger values indicate increased levels of synchronization of spiking patterns. While less accurate, this synchrony index is an efficient estimation of population-level synchrony that can be used as a feedback signal each step of the episode.</p> <p>The mean population-level membrane potentials, (\\(\\mu_{V_E}\\), \\(\\mu_{V_I}\\)), help define fragile vs robust network states relative to the firing threashold. Without these feedback signals, highly excitable networks exhibiting high levels of synchrony may receive additional stimulation to reduce the synchronization at the cost of destabalizing the network into runaway firing.</p> <p>The standard deviation of population-level membrane potentials, (\\(\\sigma_{V_I}\\), \\(\\sigma_{V_I}\\)), offer insight into the coherence of the network. Without these feedback signals, we may fail to disambiguate two main modes of reducing synchrony: descynchronization and silencing. The desired desynchronization involves breaking phase-locked oscillations while silencing involves entirely suppressing activity.</p> <p>The inclusion of mean and variance signals of exchitatory and inhibitory membrane potentials allows inference of the stability and coherence of the neural population, resolving ambiguities that cannot be captured by sunchrony measueres alone.</p> <p>The mean inhibitory-to-excitatory synaptic coupling value, \\(\\mu_{W_\\text{IE}}\\), of the plastic E-to-I synapse is governed by the underlying STDP model. This feedback signal represents the effective strength of the inhibitory regulation exerted on excitatory neurons, capturing slower-evolving network properties that influence stability and synchrony. Additionally, as shown in the graph below, the mean synaptic weight is highly correlated with the ground-truth synchrony measurement - the Kuramoto Order Parameter (Kuramoto, 1984; Daido, 1992; Tass, 2007; Ebert et al., 2014). Incorporating this feedback signal enables stimulation to be adapted in response to changes in synaptic interactions rather than instantaneous activity alone and serves as a secondary strong proxy for the true synchrony index.</p> <p> Correlating average synaptic weight with the Kuramoto Order Parameter. </p>"},{"location":"projects/neuroloop/#reward-function","title":"Reward Function","text":"<p>The reward function consists of a dense per-timestep component and an additional terminal penalty based on the Kuramoto Order Parameter, balancing synchronization suppression against stimulation cost:</p> \\[ R(t) =   \\begin{cases}       -\\alpha r_1 - \\beta r_2 - \\gamma r_3, &amp; t &lt; T \\\\       -\\alpha r_1 - \\beta r_2 - \\gamma r_3 - \\delta r_4, &amp; \\text{t = T}   \\end{cases} \\] <p>where \\(r_1\\), \\(r_2\\), \\(r_3\\), and \\(r_4\\) are respectively identified as \u201csynchrony,\u201d \u201csquared stimulation amplitude,\u201d \"mean synaptic weight,\" and \u201cKuramoto Order (Synchrony) Parameter.\u201d Weighting coefficients \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), and \\(\\delta\\) help control the trade-off between desynchronization performance and stimulation intensity, encouraging the agent to suppress pathological synchronization while avoiding unnecessarily large stimulation amplitudes. Additionally, the reward signal is normalized using running estimates of its mean and variance.</p> <p>The synchrony measurement used for the first reward component is the same as that described in the state space. This synchrony term directly encodes the control objective of descynchronization. Penalizing high degrees of synchrony aligns the learned objective with clinical findings that excessive population synchrony is linked to pathological motor symptoms.</p> \\[ r_1 = \\text{synchrony}_t \\] <p>The quadratic penalty on stimulation amplitude reflects constraints on DBS energy consumption and safety. Squaring the amplitude disproportionaly penalizes larger stimulation values, further encouraging minimal energy usage.</p> \\[ r_2 = a_t^2 \\] <p>The mean synaptic weight, \\(\\mu_{W_\\text{IE}}\\), is the same as that described in the state space. As mentioned in that section, this state variable captures network adaptation governed by STDP, not just instantaneous dynamics. Penalizing inhibitory-to-excitatory synaptic coupling discourages control strategies that rely on long-term synaptic adjustments rather than short-term modulation of neural activity. This promotes stimulation policies that achieve desynchronization while preserving stable network connectivity.</p> \\[ r_3 = \\mu_{W_\\text{IE}} \\] <p>The final component of the reward function is another measurement of synchrony. As mentioned in the state space, the per-step voltage-based synchrony measurement is useful for dense feedback. It is a more efficient but less accurate proxy for the true synchrony measurement. A more accurate measurement of the synchrony level can be obtained by computing the Kuramoto Order Parameter (KOP) based on the spike times of neurons in the excitatory population. As used in this work, the KOP is a spike-time phase-based approach for computing synchrony which, in DBS, is fundamentally about phase locking. It measures the degree of phase locking between oscillatory neural activity across the population. While it is more accurate, it is not feasible to compute at every timestep of the simulation. It is therefore computed at the end of each episode as a sparse reward component.</p> \\[ r_4 =   \\begin{cases}       \\text{KOP}, &amp; t = T \\\\       0, &amp; \\text{otherwise}   \\end{cases} \\] \\[ \\text{KOP}(t)e^{i\\psi(t)} = \\frac{1}{N_E} \\sum_{k=1}^{N_E} e^{i\\phi_k(t)} \\] \\[ \\phi_k(t) = \\frac{2\\pi(t_{k,i+1}-t)}{t_{k,i+1}-t_{k,i}} \\] <p>Jointly, the final reward signal is normalized using running estimates of its mean and variance, maintaining a consistent reward scale and stabalizing training.</p>"},{"location":"projects/neuroloop/#api","title":"API","text":"<p><code>class gymnasium.Env</code> <code>class dbsenv.envs.DBSEnv(gymnasium.Env)</code> <code>class dbsenv.envs.FTSTSEnv(DBSEnv)</code></p> <p>The main Gymnasium class for implementing Reinforcement Learning Agent environments.</p> <p>The class encapsulates an environment with arbitrary behind-the-scenes dynamics through the <code>step()</code> and <code>reset()</code> methods.</p> <p>The main API methods that users of this class need to know are:</p> <ul> <li><code>step()</code> - Updates an environment by taking an action and returning the   agent's observation, the reward for taking that action, whether the environment   has terminated or truncated, and auxiliary diagnostic information.</li> <li><code>reset()</code> - Resets the environment to an initial state, returning an initial observation and auxiliary diagnostic   information. Required before the first call to <code>step()</code>.</li> <li><code>render()</code> - Renders the environment to help visualize what the agent sees.</li> <li><code>close()</code> - Closes the environment and frees up resources.</li> </ul>"},{"location":"projects/neuroloop/#methods","title":"Methods","text":""},{"location":"projects/neuroloop/#dbsenvinit","title":"DBSEnv.init","text":"<pre><code>DBSEnv(\n    sim_config:   SimConfig,\n    model_class:  type[NeuralModel],\n    model_params: dict | None = None,\n    render_mode:  str | None = None\n)\n</code></pre> <p>PARAMETERS:</p> <ul> <li><code>sim_config</code> \u2013 Simulation configuration (timing, sampling, resolution)</li> <li><code>model_class</code> \u2013 Neural model class implementing the spiking dynamics</li> <li><code>model_params</code> \u2013 Optional model-specific keyword arguments</li> <li><code>render_mode</code> \u2013 Optional Gymnasium render mode</li> </ul>"},{"location":"projects/neuroloop/#dbsenvreset","title":"DBSEnv.reset","text":"<pre><code>DBSEnv.reset(\n    seed:    Optional[int] = None,\n    options: Optional[dict] = None\n) -&gt; tuple[ObsType, dict[str, Any]]:\n</code></pre> <p>Resets the environment to an initial state (reinitializing plasticity, stimulation timing, internal counters, etc.), returning an initial observation and info.</p> <p>PARAMETERS:</p> <ul> <li><code>seed (optional int)</code> \u2013 The seed that is used to initialize the environment's   PRNG (np_random) and the read-only attribute np_random_seed.</li> <li><code>options (optional dict)</code> \u2013 Additional information to specify how the   environment is reset.</li> </ul> <p>RETURNS:</p> <ul> <li><code>observation (ObsType)</code> \u2013 Observation of the initial state.</li> <li><code>info (dict)</code> \u2013 Auxiliary diagnostic information complementing the observation.</li> </ul>"},{"location":"projects/neuroloop/#dbsenvstep","title":"DBSEnv.step","text":"<pre><code>DBSEnv.step(\n    action: ActType\n) -&gt; tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]\n</code></pre> <p>Runs one timestep of the environment's dynamics using the agent's actions.</p> <p>PARAMETERS:</p> <ul> <li><code>action (ActType)</code> \u2013 an action provided by the agent to update the environment state.</li> </ul> <p>RETURNS:</p> <ul> <li><code>observation (ObsType)</code> \u2013 Population-level neural statistics</li> <li><code>reward (SupportsFloat)</code> \u2013 The reward as a result of taking the action, encouraging (de)synchronization</li> <li><code>terminated</code> \u2013 Episode termination flag</li> <li><code>truncated</code> \u2013 Truncation flag (unused)</li> <li><code>info</code> \u2013 Diagnostic data (e.g., spike timing)</li> </ul>"},{"location":"projects/neuroloop/#attributes","title":"Attributes","text":""},{"location":"projects/neuroloop/#dbsenvaction_space","title":"DBSEnv.action_space","text":"<p>The actions are continuous and normalized internally before being mapped to biophysically meaningful stimulation parameters.</p> <p>See action space</p>"},{"location":"projects/neuroloop/#dbsenvobservation_space","title":"DBSEnv.observation_space","text":"<p>Observations are aggregated over a fixed temporal window and normalized to ensure stable learning dynamics across stimulation regimes.</p> <p>See state space</p>"},{"location":"projects/parallel-fft/","title":"Parallel FFT","text":"<p> source code</p>"},{"location":"projects/parallel-fft/#overview","title":"Overview","text":"<p>Performance Analysis of Parallel FFT Implementations.</p> <pre><code>flowchart LR\n    input[\"Time-Domain Signal&lt;br&gt;&lt;br&gt;&lt;img src='/assets/code/fft/in.png' width='60'/&gt;\"]\n    fft[FFT]\n    output[\"Frequency-Domain Components&lt;br&gt;&lt;br&gt;&lt;img src='/assets/code/fft/out.png' width='60'/&gt;\"]\n\n    input --&gt; |input|fft --&gt; |output|output</code></pre> <p>ParallelFFT implements and evaluates multiple parallelization strategies for the Cooley-Tukey Fast Fourier Transform (FFT) to study the performance tradeoffs. A sequential baseline is compared against OpenMP, POSIX Threads (pthreads), and C++ <code>std::thread</code> variants, with a focus on scalability, memory behavior, and microarchitectural efficiency.</p> <p>Performance is evaluated across increasing input sizes and thread counts, analyzing runtime, speedup, cache behavior, and branch behavior to highlight how each threading model interacts with modern CPU architectures.</p> <p>See the What is the FFT section below for a brief primer on the Fast Fourier Transform and its use cases. I have another project that shows the FFT in action, transforming speech into feature-rich frequency bins for a simple FPGA Speech Recognizer!</p> <p>FFT Implementations:</p> <ul> <li><code>seq</code> - Sequential baseline</li> <li><code>omp</code> - OpenMP</li> <li><code>pth</code> - POSIX Threads (pthreads)</li> <li><code>cth</code> - C++ <code>std::thread</code></li> </ul> System Configuration <pre><code>% sysctl -a | grep cache\n...\nhw.perflevel0.l1icachesize: 196608\nhw.perflevel0.l1dcachesize: 131072\nhw.perflevel0.l2cachesize:  16777216\nhw.perflevel1.l1icachesize: 131072\nhw.perflevel1.l1dcachesize: 65536\nhw.perflevel1.l2cachesize:  4194304\n...\nhw.cachelinesize: 128\nhw.l1icachesize:  131072\nhw.l1dcachesize:  65536\nhw.l2cachesize:   4194304\n...\n</code></pre> <ul> <li>Apple M2 (ARM64 Silicon)</li> <li>Cores:<ul> <li>10 total</li> <li>6 performance</li> <li>4 efficiency</li> </ul> </li> <li>Cache (perf-cores):<ul> <li>L1I: 192 KB</li> <li>L1D: 128 KB</li> <li>L2: 16 MB</li> </ul> </li> </ul>"},{"location":"projects/parallel-fft/#results","title":"Results","text":"<p>Vertical lines approximate cache expenditure.</p> <p>Runtime </p> <p>Speedup </p> <p>Cache Misses </p> <p>Branch Mispredictions </p> <p>Runtime vs. Cache Misses </p> <p>Speedup vs. Thread Count </p>"},{"location":"projects/parallel-fft/#key-components","title":"Key Components","text":""},{"location":"projects/parallel-fft/#fft-implementations","title":"FFT Implementations","text":"<p><code>seq::fft</code> - sequential baseline <code>omp::fft</code> - parallel OpenMP <code>pth::fft</code> - parallel pthreads <code>cth::fft</code> - parallel C++ standard threads</p> seq::fftomp::fftpth::fftcth::fft <pre><code>void fft(std::vector&lt;cd&gt; &amp;seq) {\n    const size_t n = seq.size();\n    bit_reverse_permute(seq);\n\n    for (size_t len = 2; len &lt;= n; len &lt;&lt;= 1) { // stage size\n        double ang = -2 * PI / len;\n        cd wlen(std::cos(ang), std::sin(ang)); // Euler's root of unity\n\n        for (size_t i = 0; i &lt; n; i += len) {\n            cd w(1); // twiddle factor\n            for (size_t j = 0; j &lt; len / 2; ++j) {\n                cd a = seq[i + j];\n                cd wb = seq[i + j + len / 2] * w;\n                seq[i + j] = a + wb;\n                seq[i + j + len / 2] = a - wb;\n                w *= wlen;\n            }\n        }\n    }\n}\n</code></pre> <pre><code>void fft(std::vector&lt;cd&gt; &amp;seq) {\n    const size_t n = seq.size();\n    bit_reverse_permute(seq);\n\n    for (size_t len = 2; len &lt;= n; len &lt;&lt;= 1) { // stage size\n        double ang = -2 * PI / len;\n        cd wlen(std::cos(ang), std::sin(ang)); // Euler's root of unity\n\n#pragma omp parallel for schedule(static)\n        for (long long i = 0; i &lt; static_cast&lt;long long&gt;(n); i += len) {\n            cd w(1);\n            for (size_t j = 0; j &lt; len / 2; ++j) {\n                cd a = seq[i + j];\n                cd wb = seq[i + j + len / 2] * w;\n                seq[i + j] = a + wb;\n                seq[i + j + len / 2] = a - wb;\n                w *= wlen;\n            }\n        }\n    }\n}\n</code></pre> <pre><code>inline void fft(std::vector&lt;cd&gt; &amp;seq, size_t num_threads) {\n    const size_t n = seq.size();\n    bit_reverse_permute(seq);\n\n    for (size_t len = 2; len &lt;= n; len &lt;&lt;= 1) {\n        double ang = -2 * PI / len;\n        cd wlen(std::cos(ang), std::sin(ang));\n\n        std::vector&lt;std::thread&gt; threads;\n        threads.reserve(num_threads);\n\n        size_t chunk = n / (len * num_threads);\n        if (chunk == 0) {\n            chunk = 1;\n        }\n\n        for (size_t t = 0; t &lt; num_threads; ++t) {\n            threads.emplace_back([&amp;, t]() {\n                size_t start = t * chunk * len;\n                size_t end = std::min(start + chunk * len, n);\n\n                for (size_t i = start; i &lt; end; i += len) {\n                    cd w(1);\n                    for (size_t j = 0; j &lt; len / 2; ++j) {\n                        cd u = seq[i + j];\n                        cd v = seq[i + j + len / 2] * w;\n                        seq[i + j] = u + v;\n                        seq[i + j + len / 2] = u - v;\n                        w *= wlen;\n                    }\n                }\n            });\n        }\n\n        for (auto &amp;th : threads) {\n            th.join();\n        }\n    }\n}\n</code></pre> <pre><code>inline void fft(std::vector&lt;cd&gt; &amp;seq, size_t num_threads) {\n    const size_t n = seq.size();\n    bit_reverse_permute(seq);\n\n    for (size_t len = 2; len &lt;= n; len &lt;&lt;= 1) {\n        double ang = -2 * PI / len;\n        cd wlen(std::cos(ang), std::sin(ang));\n\n        std::vector&lt;std::thread&gt; threads;\n        threads.reserve(num_threads);\n\n        size_t chunk = n / (len * num_threads);\n        if (chunk == 0) {\n            chunk = 1;\n        }\n\n        for (size_t t = 0; t &lt; num_threads; ++t) {\n            threads.emplace_back([&amp;, t]() {\n                size_t start = t * chunk * len;\n                size_t end = std::min(start + chunk * len, n);\n\n                for (size_t i = start; i &lt; end; i += len) {\n                    cd w(1);\n                    for (size_t j = 0; j &lt; len / 2; ++j) {\n                        cd u = seq[i + j];\n                        cd v = seq[i + j + len / 2] * w;\n                        seq[i + j] = u + v;\n                        seq[i + j + len / 2] = u - v;\n                        w *= wlen;\n                    }\n                }\n            });\n        }\n\n        for (auto &amp;th : threads) {\n            th.join();\n        }\n    }\n}\n</code></pre>"},{"location":"projects/parallel-fft/#benchmarking","title":"Benchmarking","text":"<p>While basic metrics such as runtime and speedup can be derived from source code, the microarchitectural metrics such as cache or branch behavior require additional benchmark harnesses such as external scripting or profilers. Given my aforementioned system configuration, I used Apple Instruments to access data provided by the PMUs. It proved to be less convenient than Linux's <code>perf</code> CLI tool, but neither virtualization nor emulation provided sufficient access to the PMUs.</p> Timing FFTs<pre><code>template &lt;typename F&gt;\ndouble time_fft(F fft_func, const std::vector&lt;cd&gt; &amp;seq, int runs = 5) {\n    double total = 0.0;\n    for (int r = 0; r &lt; runs; ++r) {\n        auto data = seq;\n\n        auto start = Clock::now();\n        fft_func(data);\n        auto end = Clock::now();\n\n        total += std::chrono::duration&lt;double&gt;(end - start).count();\n    }\n\n    return total / runs;\n}\n</code></pre>"},{"location":"projects/parallel-fft/#what-is-the-fft","title":"What is the FFT","text":"<ul> <li><code>FT</code> - Fourier Transform</li> <li><code>DFT</code> - Discrete Fourier Transform</li> <li><code>FFT</code> - Fast Fourier Transform</li> </ul> <p>The FFT is an efficient algorithm for computing the DFT, a discrete version of the FT. That is, the FFT is an efficient algorithm for converting from the time-domain to the frequency-domain.</p> <p>The FT is a transform that maps an input function to another function that describes the extent (magnitude) to which various frequencies are present in the input function. That is, the FT decomposes a signal into its corresponding frequency components.</p> <p>Example</p> <p>I press 5 notes on a piano, making a chord. The chord is a single composite frequency containing 5 pure frequencies.</p> <p>An untrained ear hears the chord as one single sound despite the 5 notes. However, when we pass it through the FT, it is able to disambiguate which 5 notes I pressed.</p> <p>This is further illustrated in the figure below.</p> <p>The idea behind the brute force implementation of such an algorithm is to simply correlate all your frequencies with the input signal - \\(O(N^2)\\).</p> <p>The FFT is able to reduce this to \\(O(N \\log N)\\) via a DPish approach to avoid recomputing overlapping complex exponentials/partial sums in a divide-and-conquer combine fashion.</p> <p>This divide-and-conquer combine approach stems from the observation that the DFT of a length-N signal can be decomposed into two length-\\(\\frac{N}{2}\\) DFTs of the even-indexed and odd-indexed samples (with appropriate twiddle factors).</p> <p> Example decomposition of an input signal (e.g., a chord) into its 5 corresponding pure frequencies. A given point in the input signal is the sum of the corresponding point in the derived pure frequencies.</p> <p> Another example, illustrating the conversion from the time to the frequency domain. Notice the peaks at 1 and 10 Hz corresponding to the 1x and 10x components of the input signal.</p> <p>This is my attempt at summarizing the hours of information I learned from the following key resources:</p> <ul> <li>https://cp-algorithms.com/algebra/fft.html</li> <li>3Blue1Brown</li> <li>Reducible FFT</li> <li>Reducible DFT</li> <li>Steve Brunton</li> </ul> <p>todo</p> <p>I want to make a diagram for the bit reversal step of the iterative FFT.</p> <p>To see the FFT in action, check out SmallTalk - my other project utilizing the FFT for feature extraction in a simple FPGA-driven Speech Recognizer.</p> <p>Note</p> <p>Typical use cases of the FFT involve smaller input sizes, typically around 512, 1024, or 2048.</p> <p>As demonstrated in this project, 1024-point FFTs never witness the advantages of parallelism as implemented here (i.e., performing butterfly operations in parallel) because thread management amortizes the overhead.</p> <p>For this reason, the FFT is almost never parallelized in this fashion. Instead, multilpe FFTs are parallelized. In multi-channel audio processing, or in multi-channel image processing (e.g., RGB), you must compute a 2D FFT on each channel. In practice, the FFT is parallelized per-channel.</p> <p>However, when implemented in hardware, the FFT does benefit from butterfly-level parallelization. Check out my project SmallTalk to learn more about this!</p>"},{"location":"projects/smalltalk/","title":"SmallTalk","text":"<p> source code</p>"},{"location":"projects/smalltalk/#overview","title":"Overview","text":"<p>FPGA-accelerated speech recognition using FFT-based feature extraction.</p> <pre><code>flowchart LR\n    input@{ shape: circle, label: \"input\" }\n    output@{ shape: dbl-circ, label: \"output\" }\n\n    subgraph smalltalk [SmallTalk FPGA]\n        sw[Embedded CPU]\n        hw[FFT Hardware]\n        db[(Reference Words)]\n\n        sw --&gt; |frames|hw --&gt; |magnitudes|sw --&gt; |feature vector|db --&gt; |prediction|sw\n    end\n\n    input --&gt; |spoken words|smalltalk --&gt; |prediction|output\n</code></pre> <p>SmallTalk implements an embedded speech recognition system that offloads computationally intensive signal processing to an FPGA while retaining flexible control logic on the embedded Nios II soft processor (CPU). Spoken audio is transformed into frequency-domain features using a hardware FFT pipeline, then matched against a small reference dictionary to classify words.</p> <p>This project explores:</p> <ul> <li>hardware / software integration</li> <li>FPGA acceleration for DSP</li> <li>Real-time audio preprocessing and inference</li> </ul> <p>A more in-depth diagram of the hardware/software components:</p> <pre><code>flowchart TB\n    input@{ shape: circle, label: \"input\" }\n    output@{ shape: dbl-circ, label: \"output\" }\n\n    subgraph smalltalk [SmallTalk]\n        subgraph hw [Hardware]\n            fft[FFT]\n        end\n\n        subgraph sw [Software]\n            adc[ADC]\n            fb[Frame Buffer]\n            db[(Ref. Words)]\n            pred[Predictor]\n        end\n\n        db[(Reference Words)]\n    end\n\n    input --&gt; |spoken words|adc\n    adc -- amplitudes --&gt; fb -- frame windows --&gt; fft -- magnitudes --&gt; pred -- feature vector --&gt; db -- matching word --&gt; pred\n    pred --&gt; |prediction|output</code></pre>"},{"location":"projects/smalltalk/#results","title":"Results","text":"<p>Warning</p> <p>todo: not sure the best way to demo results yet...</p>"},{"location":"projects/smalltalk/#key-components","title":"Key Components","text":""},{"location":"projects/smalltalk/#audio-frontend","title":"Audio Frontend","text":"<p>The FPGA features an embeded Nio II soft processor and an on-board WM8731 Audio Codec with an Analog-to-Digital Converter which digitizes the microphone input. Audio preprocessing is thus performed in a software (C++) kernel which maps the microphone's analog signal to a stream of 16b PCM amplitude samples. These samples are then buffered into fixed-size (1024 samples) frames. A Hann windowing function is applied to the audio frames to ensure time-localized frequency analysis.</p> <p>At this point, a frame of audio is represented as an <code>int16_t frame[1024]</code>. The FFT expects a sequence of complex values, so frames are converted to <code>std::complex&lt;double&gt; frame[1024]</code>.</p> <p>These complex frames are then fed into the VHDL-defined hardware components that compose the FFT - described in the next section.</p> Diagram: WM8731 Audio Codec <p></p>"},{"location":"projects/smalltalk/#fft-accelerator","title":"FFT Accelerator","text":"<p>The FFT kernel implements a simple 1024-point radix-2 iterative Cooley-Tukey FFT. Responsible for mapping time-domain samples to frequency-domain magnitudes, the hardware-defined FFT offloads the most compute-heavy stage from the CPU, enabling near real-time processing.</p> <p>The FFT kernel is composed of two components, namely the FFT Organizer and the Butterfly Unit. The FFT organizer is responsible for orchestrating / wiring together the butterfly units. It divides the input sequence (frame) - representing an \\(N\\)-point FFT - into \\(\\log_{2} N\\) stages. Each stage performs a set of \\(\\frac{N}{2}\\) independent butterfly operations which recombine the recursive work. The butterfly units are responsible for computing a butterfly operation defined by the following equations - using 2 complex adders/subtractors and 1 complex multiplier which maps to about 10 basic arithmetic units depending on your implementation of the complex multiplier.</p> <p>Butterfly operation:</p> \\[ y_0 = x_0 + x_1 w^k_n \\\\ y_1 = x_0 - x_1 w^k_n \\] <p>where \\(w^k_n = e^{-2\u03c0k/n}\\) represents the twiddle factor. It is important to note that \\(e^{-2\u03c0/n}\\) is a primitive \\(n\\)th root of 1. Using Euler's roots of unity, we can precompute all such twiddle factors for a given \\(N\\)-point FFT. These precomputed values are stored in ROM.</p> <p>1024-point FFT</p> <p>I use a frame of 1024 samples as input to the FFT. This creates \\(\\log_{2} 1024 = 10\\) stages. Each stage has \\(\\frac{1024}{2} = 512\\) butterflies.</p> <p>Thus, to compute this 1024-point FFT, the organizer must wire together \\(10 \\cdot 512 = 5120\\) butterfly units.</p> <p>Pseudocode</p> 4-point FFT<pre><code>...\narchitecture rtl of fft4 is\n    signal s1r, s1i : signed_array;\n    signal tw_r3, tw_i3 : signed(15 downto 0);\nbegin\n    -- twiddle multiply for W4^1 = -j\n    tw_r3 &lt;=  s1i(3);\n    tw_i3 &lt;= -s1r(3);\n\n    -- stage 1\n    b0: entity butterfly port map(...);\n    b1: entity butterfly port map(...);\n\n    -- stage 2 (twiddle multiply for W4^1 = -j)\n    -- (-j)*x = (xi, -xr)\n    b2: entity butterfly port map(...);\n    b3: entity butterfly port map(...);\nend architecture;\n</code></pre> Butterfly Unit<pre><code>entity butterfly is\n    port (\n        a_real, a_imag  : in signed(15 downto 0); -- A\n        b_real, b_imag  : in signed(15 downto 0); -- B\n        w_real, w_imag  : in signed(15 downto 0); -- W\n\n        y0_real, y0_imag : out signed(15 downto 0); -- A + BW\n        y1_real, y1_imag : out signed(15 downto 0); -- A - BW\n    );\nend butterfly;\n</code></pre>"},{"location":"projects/smalltalk/#feature-extraction","title":"Feature Extraction","text":"<pre><code>flowchart LR\n\n    subgraph hw [Hardware]\n        fft[FFT]\n    end\n\n    subgraph sw [Software]\n        input@{ shape: processes, label: \"Frames\" }\n        output@{ shape: processes, label: \"Feature Vectors\" }\n    end\n\n    input -- frames --&gt; fft -- raw magnitudes --&gt; output</code></pre> <p>After passing the input signal (audio frames) through the FFT, we obtain the FFT magnitudes which act as a simple spectal representation of spoken words that are robust to timing variations.</p> <p>In practice, these raw FFT magnitudes are used to compute Mel-filterbank energies or, a step further, to compute MFCCs. However, I chose to simply use the raw FFT magnitudes to analyse the accuracy of such a baseline so that I could evaluate the extent to which the extra compute for MFs or MFCCs translates to increased accuracy.</p> <p>That is, I use the raw FFT magnitudes as my feature vector.</p> <p>To illustrate the process so far (i.e., record speech, buffer frames, compute FFT), I recorded myself speaking the english vowels (a, e, i, o, u) and fed it through the pipeline. The resulting plots are shown below.</p> <p></p> <p>Top: time-domain plot (stream of amplitude values). Bottom: frequency-domain plot (windowed FFT magnitudes).</p>"},{"location":"projects/smalltalk/#predictor","title":"Predictor","text":"<pre><code>flowchart LR\n    output@{ shape: dbl-circ, label: \"output\" }\n\n    subgraph hw [Hardware]\n        fft[FFT]\n    end\n\n    subgraph sw [Software]\n        db[(Ref. Words)]\n        pred[Predictor]\n\n    end\n\n    fft -- raw magnitudes --&gt; pred -- feature vector --&gt; db -- matching word --&gt; pred --&gt; |prediction|output</code></pre> <p>The predictor compares the extracted feature vectors against a reference word database, using distance-based matching (e.g., L2 / cosine similarity), and outputs the closest matching word.</p>"},{"location":"projects/smalltalk/#why-fft-for-speech","title":"Why FFT for Speech?","text":"<p>Human hearing is largely based on characteristics derived from the frequency domain (e.g., pitch, tone, timbre, etc.). Speech signals are more structured in the frequeny-domain where different phonemes and words exhibit different spectral patterns. This makes the FFT-derived features far more discriminative than raw time-domain samples.</p> <p></p> <p>Illustration of phonemes spoken by different people (top vs. bottom).</p> <p>SmallTalk demonstrates the FFT in a practical embedded context. For a deeper dive into the FFT algorithom, check out my other project, ParallelFFT, which implements the FFT (in software) to analyze performance and parallelization.</p>"}]}